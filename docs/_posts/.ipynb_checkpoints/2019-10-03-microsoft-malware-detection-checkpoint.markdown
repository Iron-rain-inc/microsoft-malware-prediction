---
layout: post
title:  "Microsoft Malware Detection!"
date:   2019-03-10 11:59:13 +1100
categories: jekyll update
---

![airplane](../../../../../assets/malware_blog/malicious-code-4036349_960_720.jpg)
 

# Welcome
This report details the effor to create a model for detection of malware vulnerable Microsoft computers. 
The report has been broken down into the sections below for ease of reading and a complete Jupyter Notebook report can be found here. 

<details><summary><strong>Business Understanding</strong></summary>
    <p>
        <ul>        
        <br>
            
        <!-- PROJECT STATEMENT -->
        <details><summary><strong>Project Intoduction</strong></summary>        
        <br>
        Microsoft have engaged data scientists world wide to develop a model for the purpose of detecting malware vulnerable computers.<br><br>
        The intent of this project is to develop a model using the Microsoft provided dataset, this model will be trained and then implemented on provided testing data.             
        </details><br>
        
        <!-- PROJECT STATEMENT -->
        <details><summary><strong>Problem Statement</strong></summary>
        <br>
        Malware is an issue within any computing society, billions of computer around the world are vulnerable. Malware infection is a preventable problem that can be solved, on one facet, by predicting an attack prior to it happening.<br><br>
        Using the provided dataset this project will construct a model that will predict computer vulerability to a malware attack. This information could then be used for a vendor or customer to perform preventative action.        
        </details><br>
            
        <!-- PROJECT OVERVIEW -->            
        <details><summary><strong>Project Overview</strong></summary>
        <br>
        This project will use the Microsoft LightGBM model to train and predict answers with. The LightGBM model is a gradient boosting framework based on decision tree algorithms. <br><br>
        The initial sessions for this project made use of the sckit-learn DecisionTreeClassifier and AdaBoost models, the performance in both speed and accuracy was not achieving the desired levels and multiple other models were tested.<br><br>
        LightGBM became the choice for this project as it is, comparatively, extremely quick to train a model and offered inbuil GPU support to increase that time further. 
        </details><br>
        
        <!-- METRICS -->    
        <details><summary><strong>Determining Success</strong></summary>
        The end product is a machine learning model and as such machine learning metrics will be used to determine the success of the project.<br><br>
        The following measurements will be used to evaulate the model:
        <ul>
            <li>Confusion Matrix</li>
            <li>Accuracy Score</li>
            <li>F1 Score</li>
            <li>Precision Score</li>
            <li>Recall Score</li>
        </ul>
        </details>
    </ul>
    </p>
</details>

<br>

<details><summary><strong>Data Understanding and Preprocessing</strong></summary>
    <p>
        <ul>
        <br>
        
        <!-- DATA OVERVIEW -->
        <details><summary><strong>Data Overview</strong></summary>
        <br>
        The dataset provided for model training is provided by Microsoft and Kaggle for a Kaggle Competition. 
        <br>It can be found at the Kaggle Competition <a href="https://www.kaggle.com/c/microsoft-malware-prediction/data">page</a>.
        <br><br>
        Within the dataset, each row corresponds to a unique machine, these can be servers, computer, phones, tablets and other various devices accross all branches of the Microsoft Windows operating systems.<br><br>
        The <span markdown="1">`MachineIdentifier`</span> column is used to uniquely identify each machine and the <span markdown="1">`HasDetections`</span> to indicate whether or not a computer has a malware event detected on it. The remaining data points are the states and versions of related Microsoft components on each machine. <br><br>
        The data is sourced froma variety of malware machines and does not just represent Microsoft customer machines. <br><br>
        An excerpt from the competition page <i>"The sampling methodology used to create this dataset was designed to meet certain business constraints, both in regards to user privacy as well as the time period during which the machine was running."</i> explains the sampling methodology used for this data. 
        </details><br>
        
    <!-- DATA DISCOVERY -->
    <details><summary><strong>Data Discovery</strong></summary>
    <br>
    Data discovery for this project is an iterative process mixed in with the data preprocesing, this part of the report will hilight the discovery techniques used, if you would like to see the full sequence please view the Notebook.<br><br>
        <i><b>Initial Data</b></i>
        <br>
        Upon loading the data the first two pieces of information required were the column type breakdown and the missing data reports. <br>
        <table>
          <tr>
            <th>Missing Values</th>
            <th>Data Types</th>
          </tr>
          <tr>
            <td><span markdown="1">![missing_vals](../../../../../assets/malware_blog/output_16_1.png)</span></td>
            <td><span markdown="1">![datatypes](../../../../../assets/malware_blog/output_36_1.png)</span></td>
          </tr>
        </table>                    
    Missing data requires either removal from the dataset or substitution prior to any modelling occuring. <br><br>
    For the dataset in use, less than 10% data missing in a column has been deemed trivial, the colums missing greater than this are shown:<br><br>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>Missing Percentage</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>DefaultBrowsersIdentifier</th>
          <td>95.141637</td>
        </tr>
        <tr>
          <th>OrganizationIdentifier</th>
          <td>30.841487</td>
        </tr>
        <tr>
          <th>PuaMode</th>
          <td>99.974119</td>
        </tr>
        <tr>
          <th>SmartScreen</th>
          <td>35.610795</td>
        </tr>
        <tr>
          <th>Census_ProcessorClass</th>
          <td>99.589407</td>
        </tr>
        <tr>
          <th>Census_InternalBatteryType</th>
          <td>71.046809</td>
        </tr>
        <tr>
          <th>Census_IsFlightingInternal</th>
          <td>83.044030</td>
        </tr>
        <tr>
          <th>Census_ThresholdOptIn</th>
          <td>63.524472</td>
        </tr>
        <tr>
          <th>Census_IsWIMBootEnabled</th>
          <td>63.439038</td>
        </tr>
      </tbody>
    </table>
    From this brief look at the missing data it is determined that <span markdown="1">`DefaultBrowserIdentifier`</span>, <span markdown="1">`Census_ProcessorClass`</span> & <span markdown="1">`PuaMode`</span> can be dropped as there is no feasible way to keep them.<br><br>
    The next steps are to review the remaining fields to determine if they are worth keeping. The Kaggle competition data page provides a brief insight into some of the fields meanings but further investigation was warranted.<br> 
        <ul>
            <li><span markdown="1">`OrganizationIdentifier` - ID for the organization the machine belongs in, organization ID is mapped to both specific companies and broad industries.</span></li>
            <li><span markdown="1">`SmartScreen` - A cloud-based anti-phishing and anti-malware component.</span></li>
            <li><span markdown="1">`Census_InternalBatteryType` - Internal battery type</span></li>
            <li><span markdown="1">`Census_IsFlightingInternal` - Related to OS releases</span></li>
            <li><span markdown="1">`Census_ThresholdOptIn` - NA</span></li>
            <li><span markdown="1">`Census_IsWIMBootEnabled` - Allows a Windows image to be compressed, reducing usage space.</span></li>
        </ul>
    Of the fields listed above the <span markdown="1">`SmartScreen`</span> column leaps out as important, the <span markdown="1">`SmartScreen`</span> column will be kept by dropping all rows with missing values.<br>
    <br>
    The dataset provided also has a number of fields that repeat data, whether in whole or sections of another column. The tool used to discover this is correlation and the chart can be seen below.<br><br>
    Note, many of the columns below could not be correlated until after label encoding has taken place, this is preprocessing step that will be described later.<br>
    <span markdown="1">![missing_vals](../../../../../assets/malware_blog/output_43_0.png)</span>          
    <br>
    As can be seen by the chart above there are a few columns with very high correlation values. These value require further investigation, the table below shows all columns with a correlation of 0.9 or higher. <br>
    <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>Correlation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>EngineVersion</th>
      <th>AvSigVersion</th>
      <td>0.901399</td>
    </tr>
    <tr>
      <th>AvSigVersion</th>
      <th>EngineVersion</th>
      <td>0.901399</td>
    </tr>
    <tr>
      <th>Platform</th>
      <th>OsVer</th>
      <td>0.999470</td>
    </tr>
    <tr>
      <th>Processor</th>
      <th>Census_OSArchitecture</th>
      <td>0.995984</td>
    </tr>
    <tr>
      <th>OsVer</th>
      <th>Platform</th>
      <td>0.999470</td>
    </tr>
    <tr>
      <th>OsBuildLab</th>
      <th>IeVerIdentifier</th>
      <td>0.951463</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">SkuEdition</th>
      <th>Census_OSEdition</th>
      <td>0.920566</td>
    </tr>
    <tr>
      <th>Census_OSSkuName</th>
      <td>0.906636</td>
    </tr>
    <tr>
      <th>IeVerIdentifier</th>
      <th>OsBuildLab</th>
      <td>0.951463</td>
    </tr>
    <tr>
      <th>Census_OSVersion</th>
      <th>Census_OSBuildNumber</th>
      <td>0.985123</td>
    </tr>
    <tr>
      <th>Census_OSArchitecture</th>
      <th>Processor</th>
      <td>0.995984</td>
    </tr>
    <tr>
      <th>Census_OSBuildNumber</th>
      <th>Census_OSVersion</th>
      <td>0.985123</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">Census_OSEdition</th>
      <th>SkuEdition</th>
      <td>0.920566</td>
    </tr>
    <tr>
      <th>Census_OSSkuName</th>
      <td>0.998320</td>
    </tr>
    <tr>
      <th rowspan="2" valign="top">Census_OSSkuName</th>
      <th>SkuEdition</th>
      <td>0.906636</td>
    </tr>
    <tr>
      <th>Census_OSEdition</th>
      <td>0.998320</td>
    </tr>
    <tr>
      <th>Census_OSInstallLanguageIdentifier</th>
      <th>Census_OSUILocaleIdentifier</th>
      <td>0.990449</td>
    </tr>
    <tr>
      <th>Census_OSUILocaleIdentifier</th>
      <th>Census_OSInstallLanguageIdentifier</th>
      <td>0.990449</td>
    </tr>
  </tbody>
</table>
    The next step is to determine which of the columns are repitition data and can be dropped, this is done through by comparing the value_counts() of each column.<br><br> 
    The following cells have been identified to be dropped during preprocessing; <span markdown="1">`SkuEdition`</span>, <span markdown="1">`Census_OSSkuName`</span>, <span markdown="1">`OsBuild`</span>, <span markdown="1">`Census_OSBuildNumber`</span>, <span markdown="1">`Processor`</span>, <span markdown="1">`Platform`</span>                
    <br>
    At this point the initial data discovery is complete and preprocessing can begin. 
    </details><br>
        
    <!-- PREPROCESSING -->
    <details><summary><strong>Data Preprocessing</strong></summary>
    <br>
    Continuing on from dropping the missing rows of <span markdown="1">`SmartScreen`</span>.
    The table below is how the missing data has changed:
    <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Missing Percentage</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>DefaultBrowsersIdentifier</th>
      <td>95.223947</td>
    </tr>
    <tr>
      <th>OrganizationIdentifier</th>
      <td>31.069661</td>
    </tr>
    <tr>
      <th>PuaMode</th>
      <td>99.974445</td>
    </tr>
    <tr>
      <th>Census_ProcessorClass</th>
      <td>99.471318</td>
    </tr>
    <tr>
      <th>Census_InternalBatteryType</th>
      <td>55.826906</td>
    </tr>
    <tr>
      <th>Census_IsFlightingInternal</th>
      <td>74.104113</td>
    </tr>
    <tr>
      <th>Census_ThresholdOptIn</th>
      <td>44.585629</td>
    </tr>
    <tr>
      <th>Census_IsWIMBootEnabled</th>
      <td>44.455400</td>
    </tr>
  </tbody>
</table>
    There have been no drastic changes in the data caused by the removal of the missing values for <span markdown="1">`SmartScreen`</span>.<br><br>
    The other field that is of interest is the <span markdown="1">`OrganizationIdentifier`</span>, investigation of this field shows that it is a map of integer to organization, the map is not available to us.<br><br>
    To make use of this column a value of 0.0 is added when no organization is present and all NaN values are replaced with it.<br><br>        
    All other columns are dropped at this point, including those identified by the correlation chart, and then any row with missing data is dropped to clean all the remaining NaN values.<br><br>
    The visualization below shows the reduction in viable data as columns and NaN values are dropped. 
        <table>
          <tr>
            <th>Data Reduction</th>
          </tr>
          <tr>
            <td><span markdown="1">![data_reduction](../../../../../assets/malware_blog/output_31_1.png)</span></td>            
          </tr>
        </table>
    At this point all missing data issues have been resolved and creation of a model can begin. 
        </details>
        </ul>
    </p>
</details>

<br>

<details><summary><strong>Data Modelling</strong></summary>
    <p>
        <ul>
        <details><summary><strong>Data Processing</strong></summary>
            Prior to modelling there are three major steps that have to take place; categorical handling, nomalization or scaling and the test train split. <br><br>
            
            <b>Label Encoding</b><br>
            Label Encoding was the method chosen for handling the categorical/object data. <br><br>
            For this dataset there was the additional complication of requiring label encoded data to be reverted to continue the data discovery process. Completing this was not as straightforward as it may have seemed and as such the steps have been documented.<br><br>
            To accomplish this the method below was used: 
        <table>
            <tr>
                <th>Encoding Process</th>
                <td><div class=" highlight hl-ipython3"><pre><span></span><span class="n">transformation_dict</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="n">LabelEncoder</span><span class="p">)</span>

<span class="n">train_data</span><span class="p">[</span><span class="n">object_list</span><span class="p">]</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="n">object_list</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">transformation_dict</span><span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
</pre></div></td>            
            </tr>
            <tr>
                <th>Decode Function</th>
                <td><div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">train_data</span><span class="p">,</span> <span class="n">col_list</span><span class="o">=</span><span class="n">object_list</span><span class="p">,</span> <span class="n">transform_dict</span><span class="o">=</span><span class="n">transformation_dict</span><span class="p">):</span>
    <span class="n">transformed_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="n">col_list</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">transform_dict</span><span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">name</span><span class="p">]</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    
    <span class="k">return</span> <span class="n">transformed_df</span>
</pre></div></td>            
            </tr>
            <tr>
                <th>Compare Function</th>
                <td><div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">corr_compare</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">col</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">())</span>
</pre></div>
</td>            
            </tr>
            <tr>
                <th>Usage</th>
                <td><div class=" highlight hl-ipython3"><pre><span></span><span class="n">to_be_dropped</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">corr_compare</span><span class="p">(</span><span class="n">decode</span><span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;EngineVersion&#39;</span><span class="p">,</span> <span class="s1">&#39;AvSigVersion&#39;</span><span class="p">]))</span>
</pre></div>
    </td>            
            </tr>
        </table>
        With the data now labelled and the correlation checks complete the redundant columns have been removed.<br><br>
        <b>Normalization</b><br>
        The only fields requiring normalization are the true number fields, fields such as <span markdown="1">`Census_TotalPhysicalRAM`</span> or <span markdown="1">`Census_PrimaryDiskTotalCapacity`</span> which represent a count of the fields subject matter.<br><br>
        The reason for normalizing these fields is the large variance between them.
        A field such as <span markdown="1">`Census_PrimaryDiskTotalCapacity`</span> is measured in MB, a standard computer could have a one terabyte drive which would result in this field being 1,048,576, where <span markdown="1">`Census_InternalPrimaryDiagonalDisplaySizeInInches`</span> would rarely exceed 40.<br><br>
            
        <b>Test Train Split</b><br>
        The last step prior to modelling is to split the data into test and train sections. The test section of the data being reserved for obtaining model accuracy and other metrics once it has been trained on the training section.<br><br>
        The <span markdown="1">`HasDetections`</span> column becomes the <span markdown="1">`y` or <span markdown="1">`labels`</span> and is dropped from the remaining data wich becomes the <span markdown="1">`x`</span>.<br><br>
        The split used to create the sets is 0.75 training to a 0.25 testing and a random_state of 0.<br><br>
        With the test train split created and stored in the appropriate variables model creation is ready to begin. 
        </details><br>
        
        
        <details><summary><strong>Implementation</strong></summary>
        Implementing LightGBM was a straightforward process, reference documentation can be found at the <a href="https://lightgbm.readthedocs.io/en/latest/Python-Intro.html">LightGBM Docs</a> site.<br><br>
        <b>Step One - Dataset</b><br>
        The first step in using a LightGBM model is to create a LightGBM dataset for use, this is done by using the <span markdown="1">`lightgbm.Dataset()`</span> function as seen below:<br><br>
            <div class=" highlight hl-ipython3"><pre><span></span><span class="n">model_data</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">free_raw_data</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model_data</span><span class="o">.</span><span class="n">save_binary</span><span class="p">(</span><span class="s1">&#39;./model_data.bin&#39;</span><span class="p">)</span>
</pre></div>
        <br>
        <b>Step Two - Parameters</b><br>
        Next is to choose the initial parameters to run the model with.<br>
        Initial parameters were chose based upon LightGBM documentation and reading through other projects that have used the model.<br><br>
        Initial Parameters:
        <table>
          <tr>
            <th>learning_rate</th>
            <td>0.001(Shrinkage rate, how delicate the model is in adapting.)</td>
          </tr>
          <tr>
            <th>boosting_type</th>
            <td>gdbt (Gradient Boosting Decision Tree.)</td>
          </tr>
          <tr>
            <th>nthread</th>
            <td>6 (How many threads are used, more equals faster training.)</td>
          </tr>
          <tr>
            <th>objective</th>
            <td>regression (Model loss function.)</td>
          </tr>
          <tr>
            <th>metric</th>
            <td>auc (Area Under Curve loss metric.)</td>
          </tr>
          <tr>
            <th>sub_feature</th>
            <td>0.5 (Feature fraction, will sample only part a sub-section of the features.)</td>
          </tr>
          <tr>
            <th>num_leaves</th>
            <td>10 (Controls model complexity.)</td>
          </tr>
          <tr>
            <th>min_data</th>
            <td>5 (Helps with over-fitting.)</td>
          </tr>
          <tr>
            <th>max_depth</th>
            <td>5 (Helps with over-fitting.)</td>
          </tr>
          <tr>
            <th>device</th>
            <td>gpu (Use GPU to do processing, detailed explantion in refinement part.)</td>
          </tr> 
        </table>
            
        <br>
        <b>Step Three - Training</b><br>
        Training the model is straightforward, the train function for LightGBM requires parameters, the training data and the number of rounds to run.<br><br>            
        <span markdown="1">`model = lgb.train(param, train_data, 100)`</span><br><br>
        With that the model is now trained.             
        <br><br>
        <b>Step Four - Prediction</b><br>        
        Performing the predictions on the test data to determine accuracy is a little more complicated.<br><br>
        The prediction takes place:<br><br>
        <span markdown="1">`y_pred = model.predict(X_test)`</span><br><br>
        The results must then be proccessed into binary so that the sklearn <span markdown="1">`accuracy_score`</span> function can be used. <br>
            <div class=" highlight hl-ipython3"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;=.</span><span class="mi">5</span><span class="p">:</span>
        <span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>  
        <span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
    
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
</pre></div>
        Which results in:
        <table>
            <tr>
                <th>Accuracy</th>
                <td>0.528106550196163</td>
            </tr>
        </table>
        
        <br>                        
        </details><br>
                        
        
        <details><summary><strong>Refinement</strong></summary>
        The first attempt of refining this model was with the sklearn GridSearchCV, due to limitations of the hardware used the grid search process continually ran into memory errors if more than two or three parameters were used. <br><br>
        The option of refining only two or three parameters at a time was discarded as too mandraulic, in addition, limited to using the CPU the GridSearchCV took up to six minutes for a single training event.<br><br>
        Looking at GPU options for LightGBM revealed it could be done by manually building the LightGBM Python packge, following the guide <a href="https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html">here.</a><br><br>
        With the GPU enabled the model training time was between 80%-90% faster which left developing an iterative training process for hyper-parameter tuning.<br><br>
        The process used was to create a dictionry of possible parameters:<br><br>
            <div class=" highlight hl-ipython3"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">params</span><span class="p">[</span><span class="s1">&#39;learning_rate&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">]</span>
<span class="n">params</span><span class="p">[</span><span class="s1">&#39;boosting_type&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;gbdt&#39;</span><span class="p">]</span>
<span class="n">params</span><span class="p">[</span><span class="s1">&#39;nthread&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">6</span><span class="p">]</span>
<span class="n">params</span><span class="p">[</span><span class="s1">&#39;objective&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;regression&#39;</span><span class="p">,</span> <span class="s1">&#39;binary&#39;</span><span class="p">]</span>
<span class="n">params</span><span class="p">[</span><span class="s1">&#39;metric&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;auc&#39;</span><span class="p">,</span> <span class="s1">&#39;binary_logloss&#39;</span><span class="p">]</span>
<span class="n">params</span><span class="p">[</span><span class="s1">&#39;sub_feature&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]</span>
<span class="n">params</span><span class="p">[</span><span class="s1">&#39;num_leaves&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">40</span><span class="p">,</span> <span class="mi">80</span><span class="p">]</span>
<span class="n">params</span><span class="p">[</span><span class="s1">&#39;min_data&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">60</span><span class="p">,</span> <span class="mi">80</span><span class="p">]</span>
<span class="n">params</span><span class="p">[</span><span class="s1">&#39;max_depth&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">50</span><span class="p">]</span>
<span class="n">params</span><span class="p">[</span><span class="s1">&#39;max_bin&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="mi">63</span><span class="p">]</span>
<span class="n">params</span><span class="p">[</span><span class="s1">&#39;device&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;gpu&#39;</span><span class="p">]</span>
</pre></div>
        Following this a list of every possible combination was created:<br><br>
            <div class=" highlight hl-ipython3"><pre><span></span><span class="n">parameters</span> <span class="o">=</span> <span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span> <span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="o">*</span><span class="n">params</span><span class="o">.</span><span class="n">values</span><span class="p">())]</span>
</pre></div>
        And then the code to iterate over the parameter list:<br><br>
            <div class=" highlight hl-ipython3"><pre><span></span><span class="n">accuracy_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">num_list</span><span class="p">:</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">lgb</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">param</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">num</span><span class="p">)</span>
    <span class="c1">#Prediction</span>
    <span class="n">y_pred</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="c1">#convert into binary values</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">&gt;=.</span><span class="mi">5</span><span class="p">:</span>       <span class="c1"># setting threshold to .5</span>
            <span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>  
            <span class="n">y_pred</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">=</span><span class="mi">0</span>
    
    <span class="n">accuracy</span><span class="o">=</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">num</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="n">accuracy_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
</pre></div>
        And finally the results were combined into a DataFrame like the one below and the highest scoring parameters chosen:<br><br>
        <table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>boosting_type</th>
      <th>device</th>
      <th>learning_rate</th>
      <th>max_bin</th>
      <th>max_depth</th>
      <th>metric</th>
      <th>min_data</th>
      <th>nthread</th>
      <th>num_leaves</th>
      <th>objective</th>
      <th>sub_feature</th>
      <th>score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>47</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>50</td>
      <td>auc</td>
      <td>80</td>
      <td>6</td>
      <td>80</td>
      <td>binary</td>
      <td>0.8</td>
      <td>0.676210</td>
    </tr>
    <tr>
      <th>63</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>50</td>
      <td>binary_logloss</td>
      <td>80</td>
      <td>6</td>
      <td>80</td>
      <td>binary</td>
      <td>0.8</td>
      <td>0.676210</td>
    </tr>
    <tr>
      <th>62</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>25</td>
      <td>binary_logloss</td>
      <td>80</td>
      <td>6</td>
      <td>80</td>
      <td>binary</td>
      <td>0.8</td>
      <td>0.676198</td>
    </tr>
    <tr>
      <th>46</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>25</td>
      <td>auc</td>
      <td>80</td>
      <td>6</td>
      <td>80</td>
      <td>binary</td>
      <td>0.8</td>
      <td>0.676198</td>
    </tr>
    <tr>
      <th>61</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>50</td>
      <td>binary_logloss</td>
      <td>60</td>
      <td>6</td>
      <td>80</td>
      <td>binary</td>
      <td>0.8</td>
      <td>0.676134</td>
    </tr>
    <tr>
      <th>45</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>50</td>
      <td>auc</td>
      <td>60</td>
      <td>6</td>
      <td>80</td>
      <td>binary</td>
      <td>0.8</td>
      <td>0.676110</td>
    </tr>
    <tr>
      <th>60</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>25</td>
      <td>binary_logloss</td>
      <td>60</td>
      <td>6</td>
      <td>80</td>
      <td>binary</td>
      <td>0.8</td>
      <td>0.676048</td>
    </tr>
    <tr>
      <th>44</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>25</td>
      <td>auc</td>
      <td>60</td>
      <td>6</td>
      <td>80</td>
      <td>binary</td>
      <td>0.8</td>
      <td>0.676048</td>
    </tr>
    <tr>
      <th>7</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>50</td>
      <td>auc</td>
      <td>80</td>
      <td>6</td>
      <td>80</td>
      <td>regression</td>
      <td>0.5</td>
      <td>0.676024</td>
    </tr>
    <tr>
      <th>6</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>25</td>
      <td>auc</td>
      <td>80</td>
      <td>6</td>
      <td>80</td>
      <td>regression</td>
      <td>0.5</td>
      <td>0.676024</td>
    </tr>
    <tr>
      <th>23</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>50</td>
      <td>binary_logloss</td>
      <td>80</td>
      <td>6</td>
      <td>80</td>
      <td>regression</td>
      <td>0.5</td>
      <td>0.676024</td>
    </tr>
    <tr>
      <th>22</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>25</td>
      <td>binary_logloss</td>
      <td>80</td>
      <td>6</td>
      <td>80</td>
      <td>regression</td>
      <td>0.5</td>
      <td>0.676024</td>
    </tr>
    <tr>
      <th>31</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>50</td>
      <td>binary_logloss</td>
      <td>80</td>
      <td>6</td>
      <td>80</td>
      <td>regression</td>
      <td>0.8</td>
      <td>0.676019</td>
    </tr>
    <tr>
      <th>30</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>25</td>
      <td>binary_logloss</td>
      <td>80</td>
      <td>6</td>
      <td>80</td>
      <td>regression</td>
      <td>0.8</td>
      <td>0.676019</td>
    </tr>
    <tr>
      <th>12</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>25</td>
      <td>auc</td>
      <td>60</td>
      <td>6</td>
      <td>80</td>
      <td>regression</td>
      <td>0.8</td>
      <td>0.676019</td>
    </tr>
    <tr>
      <th>13</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>50</td>
      <td>auc</td>
      <td>60</td>
      <td>6</td>
      <td>80</td>
      <td>regression</td>
      <td>0.8</td>
      <td>0.676019</td>
    </tr>
    <tr>
      <th>29</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>50</td>
      <td>binary_logloss</td>
      <td>60</td>
      <td>6</td>
      <td>80</td>
      <td>regression</td>
      <td>0.8</td>
      <td>0.676019</td>
    </tr>
    <tr>
      <th>15</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>50</td>
      <td>auc</td>
      <td>80</td>
      <td>6</td>
      <td>80</td>
      <td>regression</td>
      <td>0.8</td>
      <td>0.676019</td>
    </tr>
    <tr>
      <th>28</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>25</td>
      <td>binary_logloss</td>
      <td>60</td>
      <td>6</td>
      <td>80</td>
      <td>regression</td>
      <td>0.8</td>
      <td>0.676019</td>
    </tr>
    <tr>
      <th>21</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>50</td>
      <td>binary_logloss</td>
      <td>60</td>
      <td>6</td>
      <td>80</td>
      <td>regression</td>
      <td>0.5</td>
      <td>0.675966</td>
    </tr>
    <tr>
      <th>20</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>25</td>
      <td>binary_logloss</td>
      <td>60</td>
      <td>6</td>
      <td>80</td>
      <td>regression</td>
      <td>0.5</td>
      <td>0.675966</td>
    </tr>
    <tr>
      <th>4</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>25</td>
      <td>auc</td>
      <td>60</td>
      <td>6</td>
      <td>80</td>
      <td>regression</td>
      <td>0.5</td>
      <td>0.675966</td>
    </tr>
    <tr>
      <th>5</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>50</td>
      <td>auc</td>
      <td>60</td>
      <td>6</td>
      <td>80</td>
      <td>regression</td>
      <td>0.5</td>
      <td>0.675966</td>
    </tr>
    <tr>
      <th>53</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>50</td>
      <td>binary_logloss</td>
      <td>60</td>
      <td>6</td>
      <td>80</td>
      <td>binary</td>
      <td>0.5</td>
      <td>0.675888</td>
    </tr>
    <tr>
      <th>52</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>25</td>
      <td>binary_logloss</td>
      <td>60</td>
      <td>6</td>
      <td>80</td>
      <td>binary</td>
      <td>0.5</td>
      <td>0.675879</td>
    </tr>
    <tr>
      <th>37</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>50</td>
      <td>auc</td>
      <td>60</td>
      <td>6</td>
      <td>80</td>
      <td>binary</td>
      <td>0.5</td>
      <td>0.675879</td>
    </tr>
    <tr>
      <th>36</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>25</td>
      <td>auc</td>
      <td>60</td>
      <td>6</td>
      <td>80</td>
      <td>binary</td>
      <td>0.5</td>
      <td>0.675879</td>
    </tr>
    <tr>
      <th>14</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>25</td>
      <td>auc</td>
      <td>80</td>
      <td>6</td>
      <td>80</td>
      <td>regression</td>
      <td>0.8</td>
      <td>0.675745</td>
    </tr>
    <tr>
      <th>39</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>50</td>
      <td>auc</td>
      <td>80</td>
      <td>6</td>
      <td>80</td>
      <td>binary</td>
      <td>0.5</td>
      <td>0.675636</td>
    </tr>
    <tr>
      <th>38</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.10</td>
      <td>63</td>
      <td>25</td>
      <td>auc</td>
      <td>80</td>
      <td>6</td>
      <td>80</td>
      <td>binary</td>
      <td>0.5</td>
      <td>0.675636</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>65</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>50</td>
      <td>auc</td>
      <td>60</td>
      <td>6</td>
      <td>40</td>
      <td>regression</td>
      <td>0.5</td>
      <td>0.667243</td>
    </tr>
    <tr>
      <th>66</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>25</td>
      <td>auc</td>
      <td>80</td>
      <td>6</td>
      <td>40</td>
      <td>regression</td>
      <td>0.5</td>
      <td>0.667243</td>
    </tr>
    <tr>
      <th>67</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>50</td>
      <td>auc</td>
      <td>80</td>
      <td>6</td>
      <td>40</td>
      <td>regression</td>
      <td>0.5</td>
      <td>0.667243</td>
    </tr>
    <tr>
      <th>81</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>50</td>
      <td>binary_logloss</td>
      <td>60</td>
      <td>6</td>
      <td>40</td>
      <td>regression</td>
      <td>0.5</td>
      <td>0.667243</td>
    </tr>
    <tr>
      <th>82</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>25</td>
      <td>binary_logloss</td>
      <td>80</td>
      <td>6</td>
      <td>40</td>
      <td>regression</td>
      <td>0.5</td>
      <td>0.667243</td>
    </tr>
    <tr>
      <th>64</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>25</td>
      <td>auc</td>
      <td>60</td>
      <td>6</td>
      <td>40</td>
      <td>regression</td>
      <td>0.5</td>
      <td>0.667243</td>
    </tr>
    <tr>
      <th>115</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>50</td>
      <td>binary_logloss</td>
      <td>80</td>
      <td>6</td>
      <td>40</td>
      <td>binary</td>
      <td>0.5</td>
      <td>0.666971</td>
    </tr>
    <tr>
      <th>73</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>50</td>
      <td>auc</td>
      <td>60</td>
      <td>6</td>
      <td>40</td>
      <td>regression</td>
      <td>0.8</td>
      <td>0.666954</td>
    </tr>
    <tr>
      <th>72</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>25</td>
      <td>auc</td>
      <td>60</td>
      <td>6</td>
      <td>40</td>
      <td>regression</td>
      <td>0.8</td>
      <td>0.666954</td>
    </tr>
    <tr>
      <th>88</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>25</td>
      <td>binary_logloss</td>
      <td>60</td>
      <td>6</td>
      <td>40</td>
      <td>regression</td>
      <td>0.8</td>
      <td>0.666954</td>
    </tr>
    <tr>
      <th>89</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>50</td>
      <td>binary_logloss</td>
      <td>60</td>
      <td>6</td>
      <td>40</td>
      <td>regression</td>
      <td>0.8</td>
      <td>0.666954</td>
    </tr>
    <tr>
      <th>75</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>50</td>
      <td>auc</td>
      <td>80</td>
      <td>6</td>
      <td>40</td>
      <td>regression</td>
      <td>0.8</td>
      <td>0.666953</td>
    </tr>
    <tr>
      <th>74</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>25</td>
      <td>auc</td>
      <td>80</td>
      <td>6</td>
      <td>40</td>
      <td>regression</td>
      <td>0.8</td>
      <td>0.666953</td>
    </tr>
    <tr>
      <th>91</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>50</td>
      <td>binary_logloss</td>
      <td>80</td>
      <td>6</td>
      <td>40</td>
      <td>regression</td>
      <td>0.8</td>
      <td>0.666953</td>
    </tr>
    <tr>
      <th>90</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>25</td>
      <td>binary_logloss</td>
      <td>80</td>
      <td>6</td>
      <td>40</td>
      <td>regression</td>
      <td>0.8</td>
      <td>0.666953</td>
    </tr>
    <tr>
      <th>96</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>25</td>
      <td>auc</td>
      <td>60</td>
      <td>6</td>
      <td>40</td>
      <td>binary</td>
      <td>0.5</td>
      <td>0.666943</td>
    </tr>
    <tr>
      <th>97</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>50</td>
      <td>auc</td>
      <td>60</td>
      <td>6</td>
      <td>40</td>
      <td>binary</td>
      <td>0.5</td>
      <td>0.666925</td>
    </tr>
    <tr>
      <th>113</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>50</td>
      <td>binary_logloss</td>
      <td>60</td>
      <td>6</td>
      <td>40</td>
      <td>binary</td>
      <td>0.5</td>
      <td>0.666923</td>
    </tr>
    <tr>
      <th>112</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>25</td>
      <td>binary_logloss</td>
      <td>60</td>
      <td>6</td>
      <td>40</td>
      <td>binary</td>
      <td>0.5</td>
      <td>0.666845</td>
    </tr>
    <tr>
      <th>99</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>50</td>
      <td>auc</td>
      <td>80</td>
      <td>6</td>
      <td>40</td>
      <td>binary</td>
      <td>0.5</td>
      <td>0.666845</td>
    </tr>
    <tr>
      <th>98</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>25</td>
      <td>auc</td>
      <td>80</td>
      <td>6</td>
      <td>40</td>
      <td>binary</td>
      <td>0.5</td>
      <td>0.666845</td>
    </tr>
    <tr>
      <th>114</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>25</td>
      <td>binary_logloss</td>
      <td>80</td>
      <td>6</td>
      <td>40</td>
      <td>binary</td>
      <td>0.5</td>
      <td>0.666833</td>
    </tr>
    <tr>
      <th>106</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>25</td>
      <td>auc</td>
      <td>80</td>
      <td>6</td>
      <td>40</td>
      <td>binary</td>
      <td>0.8</td>
      <td>0.666694</td>
    </tr>
    <tr>
      <th>105</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>50</td>
      <td>auc</td>
      <td>60</td>
      <td>6</td>
      <td>40</td>
      <td>binary</td>
      <td>0.8</td>
      <td>0.666618</td>
    </tr>
    <tr>
      <th>120</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>25</td>
      <td>binary_logloss</td>
      <td>60</td>
      <td>6</td>
      <td>40</td>
      <td>binary</td>
      <td>0.8</td>
      <td>0.666597</td>
    </tr>
    <tr>
      <th>121</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>50</td>
      <td>binary_logloss</td>
      <td>60</td>
      <td>6</td>
      <td>40</td>
      <td>binary</td>
      <td>0.8</td>
      <td>0.666587</td>
    </tr>
    <tr>
      <th>123</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>50</td>
      <td>binary_logloss</td>
      <td>80</td>
      <td>6</td>
      <td>40</td>
      <td>binary</td>
      <td>0.8</td>
      <td>0.666563</td>
    </tr>
    <tr>
      <th>122</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>25</td>
      <td>binary_logloss</td>
      <td>80</td>
      <td>6</td>
      <td>40</td>
      <td>binary</td>
      <td>0.8</td>
      <td>0.666535</td>
    </tr>
    <tr>
      <th>107</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>50</td>
      <td>auc</td>
      <td>80</td>
      <td>6</td>
      <td>40</td>
      <td>binary</td>
      <td>0.8</td>
      <td>0.666502</td>
    </tr>
    <tr>
      <th>104</th>
      <td>gbdt</td>
      <td>gpu</td>
      <td>0.05</td>
      <td>63</td>
      <td>25</td>
      <td>auc</td>
      <td>60</td>
      <td>6</td>
      <td>40</td>
      <td>binary</td>
      <td>0.8</td>
      <td>0.666431</td>
    </tr>
  </tbody>
</table>                  
        A heatmap is then generated to show the most relevant parameters and further tuning can take place by adjusting the parameters and repeating the steps above until the desired accuracy is achieved.<br><br>
            <span markdown="1">![data_reduction](../../../../../assets/malware_blog/output_77_1.png)</span><br><br>
        Eventually this iteration leads to final parameters of: <br><br>
        <table>
          <tr>
            <th>learning_rate</th>
            <td>0.2</td>
          </tr>
          <tr>
            <th>boosting_type</th>
            <td>gdbt</td>
          </tr>
          <tr>
            <th>nthread</th>
            <td>6</td>
          </tr>
          <tr>
            <th>objective</th>
            <td>binary</td>
          </tr>
          <tr>
            <th>metric</th>
            <td>auc </td>
          </tr>
          <tr>
            <th>sub_feature</th>
            <td>0.8</td>
          </tr>
          <tr>
            <th>num_leaves</th>
            <td>240</td>
          </tr>
          <tr>
            <th>min_data</th>
            <td>80</td>
          </tr>
          <tr>
            <th>max_depth</th>
            <td>50</td>
          </tr>
          <tr>
            <th>device</th>
            <td>gpu</td>
          </tr> 
        </table>
        With an accuracy of: <br><br>
        <table>
          <tr>
            <th>Accuracy</th>
            <td>0.6823402445372709</td>
          </tr>
        </table>
        </details>        
    </ul>
    </p>
</details>

<br>

<details><summary><strong>Model Results and Evaluation</strong></summary>
    <p>
        <ul>
        <details><summary><strong>Results</strong></summary>            
            Initial Results:<br><br>
            <ul>
            <table>
                <tr>
                    <th>Accuracy</th>
                    <td>0.528106550196163</td>
                </tr>
            </table>
            </ul>
            Final Results:<br><br>
            <ul>
            <table>
                <tr>
                    <th>Accuracy</th>
                    <td>0.6823402445372709</td>
                </tr>
            </table>
            </ul>
        </details><br>
        <details><summary><strong>Evaluation</strong></summary>
            The first component of the model evaluation is a confusion matrix, below the intial vs the final can be seen:<br><br>
             <table>
                <tr>
                    <th>Initial</th>
                    <th>Final</th>                    
                </tr>
                <tr>
                    <td><span markdown="1">![missing_vals](../../../../../assets/malware_blog/output_71_1.png)</span></td>
                    <td><span markdown="1">![missing_vals](../../../../../assets/malware_blog/output_73_1.png)</span></td>
                </tr>
            </table>
            Prior to tuning the model has a tendancy towards false positive with relatively few negatives.<br>
            Post tuning the model is much more balanced and overall more accurate.<br><br>
            Below is the classification report for the initial model, the high scores on the positive hits are due to the models tendancy to mark everything as a positive, this is confirmed by looking at the low scores on the negatives.<br><br>
            <div class="output_subarea output_stream output_stdout output_text">
<pre>              precision    recall  f1-score   support

           0       0.79      0.03      0.05    569310
           1       0.52      0.99      0.69    612105

   micro avg       0.53      0.53      0.53   1181415
   macro avg       0.66      0.51      0.37   1181415
weighted avg       0.65      0.53      0.38   1181415
</pre>
</div>
           The table below is the classificatino report for the final model, the score is more balanced across positive and negative hits.<br><br>
            <div class="output_subarea output_stream output_stdout output_text">
<pre>              precision    recall  f1-score   support

           0       0.66      0.69      0.68    569310
           1       0.70      0.68      0.69    612105

   micro avg       0.68      0.68      0.68   1181415
   macro avg       0.68      0.68      0.68   1181415
weighted avg       0.68      0.68      0.68   1181415
</pre>
</div>
            
        </details>
    </ul>
    </p>
</details>

<br>

<details><summary><strong>Conclusion</strong></summary>
    <p>
        <ul>
        <details><summary><strong>Reflection</strong></summary>
            This project set out to provide predictions on a dataset containing client computers.<br><br>
            The supplied data has been explored, cleaned, encoded and normalized for use in training the model.<br><br>
            The predictions are to determine whether a machine is vulerable to a malware infection. Through the use of the LightGBM model this project has provided a trained model and processes required to complete the goal.<br><br>
            An interesting component of this project was learning and developing skills on the LightGBM model while the most difficult components was tuning the hyper-parameters.<br><br>
            Learning a model from scratch, not taught as part of the Udacity curriculum, provided insight in how to research and deploy models for problems not encountered as part of the course. <br><br>
            Tuning the hyper-parameters was a useful exercise in multiple skills; building Python packages, installing a CUDA supporting environment to enabel GPU use in Jupyter Notebook, developing the code required to iterate, store and determing the most useful parameters. Each step of setting up the parameter tuning resulted in new problems and solutions to solve and apply.            
        </details><br>
        <details><summary><strong>Improvement</strong></summary>
            An improvement that could be used in this implementation is a pipeline for preparing data through to obtaining accuracy scores.<br><br>
            This would allow for much greater flexibility in making changes to the data preprocessing and could have an affect on the final model scores.<br><br>
            Currently if changes to the preprocessing are to be made they would flow through and affect every part of the final model. 
        </details>        
    </ul>
    </p>
</details>
